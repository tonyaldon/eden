* Eden is a simple ChatGPT client for Emacs that focuses on conversations.

*Eden* supports OpenAI, Anthropic, Deepseek, Perplexity, and X.ai APIs.
For reasoning models, you can choose whether or not to display the
reasoning.  For Perplexity and OpenAI web search, while citations are
inlined, you can also list them in a dedicated buffer.

*Eden*'s interface is simple:

- You want to ask something to *ChatGPT*?  Call ~eden~ command, enter your
  prompt, press ~C-c C-c~ and you're done.

  #+html: <p align="center">
  #+html: <img src="https://raw.githubusercontent.com/tonyaldon/eden/master/img/demo.gif" />
  #+html: </p>

- You want to integrate the response in your ~org-mode~ notes?  Just
  copy/paste it.
- You want to continue a conversation saved in your ~org-mode~ notes?
  On the request at point, call ~eden~ with the ~C-u~ prefix argument
  which opens a transient menu, then press ~c~ to to continue the
  conversation whose last request is the request at point.

  #+html: <p align="center">
  #+html: <img src="https://raw.githubusercontent.com/tonyaldon/eden/master/img/simple-conversation-req-at-point.png" />
  #+html: </p>

- You want to continue a conversation from a previous request?
  Simple!  In the prompt buffer, navigate through the prompt history
  using ~M-p~ and ~M-n~ to find the desired request, open the menu calling
  ~eden~ and press ~c~ to continue the conversation from that request.

  #+html: <p align="center">
  #+html: <img src="https://raw.githubusercontent.com/tonyaldon/eden/master/img/simple-conversation-prompt-buffer.png" />
  #+html: </p>

- You want to switch the API and the model?  Just call ~eden~ in the
  prompt buffer, then press ~a~ to change the API and ~m~ to change the
  model.

  #+html: <p align="center">
  #+html: <img src="https://raw.githubusercontent.com/tonyaldon/eden/master/img/simple-configuration-api-model.png" />
  #+html: </p>

- You want to read the reasoning behind the response?  Great!  In the
  prompt buffer, call ~eden~ and press ~i~ to toggle the reasoning.  Send
  your request and get the reasoning in its own section.

  #+html: <p align="center">
  #+html: <img src="https://raw.githubusercontent.com/tonyaldon/eden/master/img/simple-configuration-include-reasoning.png" />
  #+html: </p>

- You want to list Perplexity citations?  Fine!  On the request at
  point, call ~eden~ with the ~C-u~ prefix argument, then press ~C~.

  #+html: <p align="center">
  #+html: <img src="https://raw.githubusercontent.com/tonyaldon/eden/master/img/simple-citations.png" />
  #+html: </p>

Requests are independent by default and are appended to the
~*eden[requests]*~ buffer.  To link them together, you must be in a
conversation; in that case, they are appended to unique buffers named
like this: ~*eden[<conversation's name>]*~.

Note that all requests are stored in the ~eden-dir~ directory, which
defaults to the ~eden~ subdirectory within your ~user-emacs-directory~.

* Get started in minutes

1) Ensure the following utilities are installed and present in one
   of your ~exec-path~ directories:

   - ~curl~
   - ~uuidgen~
   - ~pandoc~

   You can ask ChatGPT to help you with this (replace ~distribution~ with
   your distribution be Mac, Ubuntu, etc.):

   #+BEGIN_SRC text
   How to install the utilities curl, uuidgen, and pandoc on <distribution>?
   #+END_SRC

2) Add the directory containing ~eden.el~ to your ~load-path~ and
   require the *Eden* package by adding the following lines to your init
   file, ensuring to replace ~/path/to/eden/~ with the appropriate
   directory:

   #+BEGIN_SRC emacs-lisp
   (add-to-list 'load-path "/path/to/eden/")
   (require 'eden)
   #+END_SRC

   Or if you're using ~straight~:

   #+BEGIN_SRC emacs-lisp
   (straight-use-package
    '(eden :type git :host github :repo "tonyaldon/eden"))
   #+END_SRC

3) Store your OpenAI API key in either the ~/.authinfo.gpg file
   (encrypted with gpg) or the ~/.authinfo file (plaintext):

   - After funding your [[https://platform.openai.com][OpenAI account]] ($5.00 is enough to get
     started), create an OpenAI API key visiting
     https://platform.openai.com/api-keys.  You can check [[https://www.youtube.com/watch?v=oXoFZIcRvJU][my video]] if
     you've never done this before.
   - Add the API key in the selected file as follows:

     #+BEGIN_SRC authinfo
     machine openai password <openai-api-key>
     #+END_SRC

     where ~<openai-api-key>~ is your API key.

   - Restart Emacs to apply this change.

4) Call the command ~eden~ to switch to ~*eden*~ prompt buffer,
5) Enter your prompt,
6) Press ~C-c C-c~ to send your prompt to OpenAI API,
7) Finally, the response will asynchronously show up in a dedicated
   buffer upon receipt.

* Adding API keys to ~/.authinfo or ~/.authinfo.gpg

To use the APIs from OpenAI, Anthropic, Deepseek, Perplexity, or X.ai
you need to store their API key in either the ~/.authinfo.gpg file
(encrypted with gpg) or the ~/.authinfo file (plaintext) as follow:

#+BEGIN_SRC authinfo
machine openai password <openai-api-key>
machine anthropic password <anthropic-api-key>
machine deepseek password <deepseek-api-key>
machine perplexity password <perplexity-api-key>
machine x.ai password <x.ai-api-key>
#+END_SRC

You can create new API keys at the following links:

- https://platform.openai.com/api-keys
- https://console.anthropic.com/settings/keys
- https://platform.deepseek.com/api_keys
- https://perplexity.ai/account/api/keys
- https://console.x.ai

* Managing conversations

*Eden* focuses on conversations without enforcing them; defaulting to
independent requests, it makes starting new conversations or
continuing from previous ones easy!

There are several ways to engage in a conversation while in the prompt
buffer:

1) To start a new conversation, call the ~eden~ command, press ~n~,
   and enter a title.  This creates a new empty conversation, setting
   it as the current one for all new requests.

2) To start a conversation from the current request in history
   (excluding earlier exchanges), navigate through the history using
   ~M-p~ and ~M-n~ to find the desired request.  Then, call the ~eden~
   command, press ~s~, and enter a title.  This creates a new
   conversation that already include one exchange.

3) To continue an existing conversation call the ~eden~ command, press
   ~c~, and enter a title.  This will include all previous exchanges of
   the current request in history.

You can pause the current conversation by calling ~eden~ and pressing
~SPC~.  Subsequent requests will then be independent again.

When you are in a conversation, the name of the conversation appears
in the mode line of the prompt buffer, enclosed in brackets.

Note that conversation titles and IDs are not stored; they only exist
during your Emacs session.  However, you can retrieve any conversation
later either by saving it in your notes (with its UUID) or navigating
the prompt history with ~M-p~ and ~M-n~.

* All requests are stored on disk

All requests are stored in the ~eden-dir~ directory, which defaults to
the ~eden~ subdirectory within your ~user-emacs-directory~, providing a
range of benefits:

1) Requests are always preserved, ensuring you can retrieve them at
   any time.
2) With the request's UUID, you can track down the associated
   request and check details like the API, model, system prompt, and
   timestamp.
3) Should an error occur during processing, the corresponding
   ~error.json~ file can be consulted for troubleshooting.
4) You can start or continue a conversation from any existing request
   (a feature known as "branching"):
   - Either from a request at point in your notes,
   - Or navigating through history in the prompt buffer using ~M-p~ and
     ~M-n~ to find the desired request, opening the menu with ~eden~, and
     pressing ~c~ to continue the conversation or ~s~ to start a new
     conversation from the request.
5) All data is stored in JSON (or text format), facilitating
   integration with other software for further analysis.

* Configuring Eden

In the prompt buffer, you can call ~eden~ and press ~C~ to show the
current configuration.

From the menu provided by ~eden~, you can modify the current
configuration doing the following

- Press ~a~ to set the current API (~eden-api-set~),
- Press ~m~ to set the model for the current API (~eden-model-set~),
- Press ~t~ to set the temperature (~eden-temperature-set~),
- Press ~i~ to include reasoning information (~eden-include-reasoning-toggle~),
- Press ~o~ to access another menu with more options (~eden-more-options-menu~),

or the following for modifying the system message:

- Press ~S~ to add a system message (~eden-system-message-add~),
- Press ~u~ to update the system message (~eden-system-message-update~),
- Press ~p~ to pick a system message (~eden-system-message-set~),
- Press ~r~ to reset the system message (~eden-system-message-reset~).

** Navigating the profile history

Each time you quit Eden's menu, the current *profile* which includes

- the API,
- the request directory,
- the model,
- if you include the reasoning,
- the temperature,
- the current conversation if any, and,
- the system message if any

is pushed to an history that you can navigate from the prompt buffer
with:

- ~C-M-p~ bound to ~eden-profile-previous~ and
- ~C-M-n~ bound to ~eden-profile-next~.

** The complete list of user variables

The complete list of user variables you may want to adjust includes:

- ~eden-api~
- ~eden-apis~
- ~eden-model~
- ~eden-temperature~
- ~eden-system-message~
- ~eden-system-messages~
- ~eden-system-message->developer-for-models~
- ~eden-dir~
- ~eden-anthropic-max-tokens~
- ~eden-anthropic-thinking-budget-tokens~
- ~eden-web-search-context-size~
- ~eden-org-property-date~
- ~eden-org-property-model~
- ~eden-org-property-req~
- ~eden-pops-up-upon-receipt~
- ~eden-include-reasoning~
- ~eden-prompt-buffer-name~

For more information on these variables, consult their documentation
in the ~*Help*~ buffer using ~describe-variable~ command, bound by default
to ~C-h v~.

* Alternatives

See "Alternatives" section of [[https://github.com/karthink/gptel][gptel]] README for a comprehensive list of
Emacs clients for LLMs not limited to OpenAI.

* FAQ
** Do you support streaming?

No.

** Why don't you support streaming?

I don't like it.

Streaming the response forces me to read it immediately and linearly.

That's not how I read.  I often start from the end and go backward,
picking out only the pieces I'm interested in.  If I need a more
profound understanding of the answer, I might then read it linearly to
make sure I don't miss anything.

And if I have to read the entire text of each response, I'll get
exhausted too quickly.  My processing power can't keep up with the
production rate of LLMs.  I have to choose wisely what I read and what
I don't.

You might say, "Nobody is forcing you to read it this way; you can just
wait until the end." That's true!  But in that case, why bother
implementing streaming at all?
